# Local CLI Document Chat Application – Final Specifications

## 1. Overview
A Python-based command-line interface (CLI) tool that enables users to query local documents and receive responses generated by a **local Large Language Model (LLM)** using **Retrieval Augmented Generation (RAG)**.

## 2. Core Objectives
- **Conversational Access**: Natural language interaction with local document collections.
- **Local-Only Operation**: No internet dependency for inference or processing.
- **Incremental Processing**: Process only new or changed documents.
- **Persistent Context**: Retain embeddings, metadata, and chat history across sessions.

---

## 3. Functional Requirements

### 3.1 Document Source
- CLI argument for folder path; defaults to `./docs` if omitted.

### 3.2 Change Tracking
- **Database**: SQLite with:
  - File path
  - SHA-256 content hash
  - Last modified timestamp
- On startup:
  - **New files** → parse, chunk, embed.
  - **Modified files** → re-process.
  - **Deleted files** → remove from vector DB.

### 3.3 Supported Formats
- `.txt` – Plain text
- `.md` – Markdown
- `.pdf` – PDF
- `.csv` – Comma-separated values

### 3.4 Error Handling
- Log errors per file without stopping other processing.

---

## 4. Retrieval Augmented Generation (RAG)

### 4.1 Text Chunking
- Recursive character split (default: 1,000 chars, 200 overlap).
- Configurable in `config.yaml`.

### 4.2 Embeddings
- **Model**: `all-MiniLM-L6-v2` via `sentence-transformers`.
- **Vector Store**: ChromaDB (persistent).

### 4.3 Query Flow
1. User question → embedding.
2. Search ChromaDB for top 4 similar chunks.
3. Build prompt with chunks + question.
4. Send to local LLM.
5. Display answer with source filenames.

---

## 5. Interactive CLI Chat
- Continuous loop until `exit`/`quit`.
- Save daily history in `./history/chat_history_YYYY-MM-DD.txt`.
- Display sources alongside answers.

---

## 6. Recommended Local Model
- **Model**: `Mistral-7B-Instruct-v0.2-GGUF` (Q4_K_M or Q5_K_M).
- **Backend**: `llama-cpp-python` with GPU acceleration if available.

---

## 7. Technical Decisions & Rationale

### Language
- **Python 3.11+** – strong ecosystem for NLP, CLI tools, and portability.

### CLI Framework
- **Typer** – easy to extend, automatic help text.

### Document Parsing
- `.txt`/`.md`: built-in file handling + `markdown` lib if needed.
- `.pdf`: `pypdf`.
- `.csv`: built-in `csv` module.
- Modular parser architecture.

### Change Detection
- SHA-256 hashes via `hashlib` for collision safety.

### Embedding Search
- ChromaDB with cosine similarity.

### Conversation History
- Plain UTF-8 `.txt` files (readable, searchable).

### Configuration
- `config.yaml` with paths, chunking params, model settings.

### Logging
- Python `logging` module, rotating logs.

### Environment & Packaging
- Poetry for dependency management.
- Virtual environment isolation.

### Portability
- Cross-platform (Windows/macOS/Linux).
- Use `pathlib` for filesystem paths.

---

## 8. Non-Functional Requirements
- **Performance**: Ingest 100 small docs in under 2 minutes.
- **Security**: No internet calls by default.
- **Portability**: Run without code changes across OSes.

---

## 9. High-Level Architecture


```
[CLI Input]
    ↓
[Query Processor] → [Embedding Model] → [Vector Store (ChromaDB)]
    ↓                              ↑
[Prompt Builder] ——————[Retrieved Chunks]
    ↓
[Local LLM] → [Answer + Sources]
    ↓
[CLI Output & History Logger]
```
